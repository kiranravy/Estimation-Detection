\documentclass[a4paper,english,12pt]{article}
%\input{header}
\title{Lecture 23: Kalman Filtering}
\date{March 31, 2016}
\author{}
\usepackage{amsmath}
\begin{document}
\maketitle
\maketitle
%\usepackage{geometry}

%\geometry{top=2cm,bottom=2cm,left=3cm,right=2cm}


 \noindent
 %\textbf{\huge{KALMAN FILTERING}}\\\\
 \section{Linear Dynamic System}\\\\

 % \begin{subequations}
  \noindent
  
 $ X_t_+_1 = F_t X_t + G_t U_t \\\\
  
  Y_t = H_tX_t + V_t \\\\
  
  X_0 \sim N(m_0,\mathbf{\Sigma}_0) \\\\$

 % \end{subequations}


  $\{U_t\}$ and $\{V_t\}$ are independent sequence of independent zero mean Gaussian.\\\\
  $Cov(U_t)=Q_t, Cov(V_t)=R_t$\\\\
  \underline{Filtering} : Estimate $X_t$ given $(Y_0,...,Y_t)$ \equiv $Y_0^t$\\\\
  \underline{Prediction} : Estimate $X_{t+1}$ given $(Y_0,...,Y_t)$ \equiv $Y_0^t$\\\\
  \underline{Criterion} : Minimise mean square error of estimator $\hat X_{t|t} : \mathbb{E}[||\hat X_{t|t} - X_t||_2^2]$\\\\
  \underline{Conceptual Solution}\\\\
  Parametric Estimation, where
  $\textbf{Y}=(\underline{Y_0},\cdots,\underline{Y_t})=\underline{Y_0^t}\sim f(y|\theta)$
  where $\Theta \equiv \underline{X_0^t} \equiv (\underline{X_0},\cdots,\underline{X_t})$
  Prior on $\Theta$ is the distribution of $\underline{X_0^t}$ induced by the distribution of 
  $\underline{X_0}$ and $\{\underline{U_n}\}_{n=0}^t$\\\\
  
  $\textbf{Y}\sim f(y|\theta)$\\
  
  $\textbf{Y} = \textbf{H}\Theta + \textbf{V}$\\
  
  \begin{equation*}
   \left[\begin{array}{c}
         \underline{Y_0}\\
         \vdots\\
         \colon\\
         \underline{Y_t}
        \end{array}\right] = 
       \left[\begin{array}{cccc}
         H_0\\
         \ & H_1\\
         \ &\ &\ddots\\
         \ &\ &\ &H_t
        \end{array}\right] 
        \left[\begin{array}{c}
         \underline{X_0}\\
         \vdots\\
         \colon\\
         \underline{X_t}
        \end{array}\right] + 
        \left[\begin{array}{c}
         \underline{V_0}\\
         \vdots\\
         \colon\\
         \underline{V_t}
        \end{array}\right]
 \end{equation*}\\
  Loss function $L(g(\theta),\textbf{W(\textbf{Y}))} = ||\theta_t - \textbf{W(\textbf{Y}}))||_2^2$\\
  From Bayesian estimation, the best estimator is $\mathbb{E}[g(\theta)|\textbf{Y}]=
  \mathbb{E}[\underline{X_t}|\underline{Y_0}\cdots\underline{Y_t}]$\\\\
  \underline{Theorem :} Diiscrete time Kalman-Bucy Filter\\
  For the linear dynamical system * , the optimal squared error estimates\\\\
  $\underline{\hat X}_{t|t}\ := \mathbb{E}[\underline{X}_t|\underline{Y}_0^t],\ &\\\\
   \underline{\hat X}_{t+1|t}\ := \mathbb{E}[\underline{X}_{t+1}|\underline{Y}_0^t]$
\\\\obey the following recursions:\\\\
  $(M1)\ \ \ \underline{\hat X}_{t|t}=\underline{\hat X}_{t|t-1} + \textbf{K}_t(\underline{Y}_t-\textbf{H}_t\underline{\hat X}_{t|t-1}, t=0,1,\cdots,$\\
  $(T1)\ \ \ \underline{\hat X}_{t+1|t}=\textbf{F}\underline{\hat X}_{t|t}$
\\\\with initialization:
$\underline{\hat X}_{0|-1}=\underline{m}_0=\mathbb{E}[\underline{X}_0],$
$\textbf{K}_t:= \mathbf{\Sigma}_{t|t-1} \textbf{H}_t^T(\textbf{H}_t \mathbf{\Sigma}_{t|t-1}\textbf{H}_t^T+\textbf{R}_t)^{-1}$
\\\\where $\mathbf{\Sigma}_{0|-1} := \mathbf{\Sigma}_0, &
\mathbf{\Sigma}_{t|t-1} := Cov(\underline{X}_t|\underline{Y}_0^t)=Cov((\underline{X}_t-\underline{\hat X}_{t|t-1})|\underline{Y}_0^t)$\\
\\\\More over, the covariance matrices satisfy the recursion \\\\
$(M2)\ \ \ \ \mathbf{\Sigma}_{t|t}=\mathbf{\Sigma}_{t|t-1}-\textbf{K}_t\textbf{H}_t\mathbf{\Sigma}_{t|t-1}$\\\\
$(T2)\ \ \ \ \mathbf{\Sigma}_{t+1|t}=\textbf{F}_t\mathbf{\Sigma}_{t|t}\textbf{F}_t^T + \textbf{G}_t\textbf{Q}_t\textbf{G}_t^T\\\\
\underline{X}_{t+1}=\textbf{F}_t\underline{X}_t+\textbf{G}_t\underline{U}_t\\\\
\underline{Y}_t=\textbf{H}_t\underline{X}_t+\underline{V}_t$\\\\
\underline{Remark:}\\\\
K-B Filter gives a sequential rule to output estimates.\\\\
\underline{Proof:}\\\\
Lets show (T1) and (T2):\\\\

%\begin{equation*}
% \begin{align}
  $\underline{\hat X}_{t+1|t}&=\mathbb{E}[\underline{X}_{t+1}|\underline{Y}_0^t]\\\\
  =\mathbb{E}[\textbf{F}_t\underline{X}_t+ \textbf{G}_t\underline{U}_t|\underline{Y}_0^t]\\\\
  =\textbf{F}_t\mathbb{E}[\underline{X}_t|\underline{Y}_0^t]+\textbf{G}_t\mathbb{E}[\underline{U}_t|\underline{Y}_0^t]\\\\
 =\textbf{F}_t\underline{\hat X}_{t|t}\\\\
 \mathbf{\Sigma}_{t+1|t}=Cov(\underline{X}_{t+1}|\underline{Y}_0^t)\\\\
 =Cov(\textbf{F}_t\underline{X}_t+\textbf{G}_t\underline{U}_t|\underline{Y}_0^t)\\\\
 =Cov(\textbf{F}_t\underline{X}_t|\underline{Y}_0^t)+Cov(\textbf{G}_t\underline{U}_t|\underline{Y}_0^t)\\\\
 =\textbf{F}_tCov(\underline{X}_t|\underline{Y}_0^t)\textbf{F}_t^T+\textbf{G}_tCov(\underline{U}_t|\underline{Y}_0^t)\textbf{G}_t^T\\\\\
 =\textbf{F}_t\mathbf{\Sigma}_{t|t}\textbf{F}_t^T+\textbf{G}_t\textbf{Q}_t\textbf{G}_t^T$\\\\
\noindent
 \underline{Proof of (M1)}\\\\
 \underline{Lemma:}\\\\
 Suppose $A \in \mathbb{R}^n, B \in\mathbb{R}$ are jointly Gaussian random vectors with $\mathbb{E[A]=\underline{\mu}}_A,
 \mathbb{E}[B]=\underline{\mu}_B, \\\\
 Cov(A)=\mathbf{\Sigma}_A, Cov(B)=\mathbf{\Sigma}_B\\\\
 Cov(A,B)=\mathbb{E}[(A-\underline{\mu}_A)(B-\underline{\mu}_B)^T]\\\\
 =\mathbf{\Sigma}_{AB}=\mathbf{\Sigma}_{BA}^T$\\\\
 Then the conditional distribution of $B$ given $A$ is (multivariate) Gaussian, with mean:\\\\
 $\underline{\mu}_{B|A}=\underline{\mu}_B+\mathbf{\Sigma}_{BA}+\mathbf{\Sigma}_A^{-1}(A-\underline{\mu}_A) \\\\ $\& covariance :
 \mathbf{\Sigma}_{B|A}=\mathbf{\Sigma}_B-\mathbf{\Sigma}_{BA}\mathbf{\Sigma}_A^{-1}\mathbf{\Sigma}_{AB}
 
 
 
 %\end{align}
%\end{equation*}

 
\end{document}
